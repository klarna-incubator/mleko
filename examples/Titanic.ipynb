{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `mleko` with Titanic\n",
    "\n",
    "This notebook is a quick introduction to `mleko` package. We will use the Titanic dataset to predict whether a passenger survived or not.\n",
    "\n",
    "The library provides 2 subpackages needed for data processing and model training: \n",
    "- `dataset`: Subpackage for handling and processing datasets.\n",
    "  - `ingest`: Module for ingesting (loading) data from various sources.\n",
    "    - `BaseIngester`: Base class for all ingesters.\n",
    "    - `KaggleIngester`: Ingests data from Kaggle.\n",
    "    - `S3Ingester`: Ingests data from Amazon S3.\n",
    "  - `convert`: Module for converting data into different formats.\n",
    "    - `BaseConverter`: Base class for all converters.\n",
    "    - `CsvToVaexConverter`: Converts CSV data into a Vaex DataFrame.\n",
    "  - `split`: Module for splitting datasets into training and testing sets.\n",
    "    - `BaseSplitter`: Base class for all splitters.\n",
    "    - `RandomSplitter`: Splits data randomly.\n",
    "    - `ExpressionSplitter`: Splits data based on a given expression.\n",
    "  - `transform`: Module for transforming datasets.\n",
    "    - `BaseTransformer`: Base class for all transformers.\n",
    "    - `CompositeTransformer`: Combines multiple transformers.\n",
    "    - `FrequencyEncoderTransformer`: Encodes categorical variables based on their frequency.\n",
    "    - `LabelEncoderTransformer`: Encodes categorical variables with unique labels.\n",
    "    - `MaxAbsScalerTransformer`: Scales each feature by its maximum absolute value.\n",
    "    - `MinMaxScalerTransformer`: Scales each feature to a given range.\n",
    "  - `feature_select`: Module for feature selection.\n",
    "    - `BaseFeatureSelector`: Base class for all feature selectors.\n",
    "    - `CompositeFeatureSelector`: Combines multiple feature selectors.\n",
    "    - `InvarianceFeatureSelector`: Selects features based on their invariance.\n",
    "    - `MissingRateFeatureSelector`: Selects features based on their missing rate.\n",
    "    - `PearsonCorrelationFeatureSelector`: Selects features based on their Pearson correlation.\n",
    "    - `VarianceFeatureSelector`: Selects features based on their variance.\n",
    "- `model`: Subpackage for building and training models.\n",
    "  - `BaseModel`: Base class for all models.\n",
    "  - `LGBMModel`: Trains a LightGBM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "This section contains configurations for the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "Define various constants that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle dataset identifier\n",
    "OWNER_SLUG = \"yasserh\"\n",
    "DATASET_SLUG = \"titanic-dataset\"\n",
    "DATASET_NAME = f\"{OWNER_SLUG}/{DATASET_SLUG}\"\n",
    "\n",
    "# Define meta features of the dataset not used as model inputs\n",
    "TARGET_FEATURE = \"Survived\"\n",
    "ID_COLUMN = \"PassengerId\"\n",
    "META_FEATURES = [ID_COLUMN, TARGET_FEATURE]\n",
    "\n",
    "# General Configuration\n",
    "RANDOM_STATE = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "In this cell, we use the `KaggleIngester` from the `mleko` library to download the Titanic dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.dataset.ingest import KaggleIngester\n",
    "\n",
    "\n",
    "# Fetch data from Kaggle and return paths to the downloaded files\n",
    "csv_paths = KaggleIngester(\n",
    "    destination_directory=f\"data/{DATASET_NAME}/raw\", \n",
    "    owner_slug=OWNER_SLUG, \n",
    "    dataset_slug=DATASET_SLUG\n",
    ").fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fetching Data from S3\n",
    "In addition to the `KaggleIngester`, `mleko` also provides the `S3Ingester` for downloading datasets from Amazon S3.\n",
    "\n",
    "Here's an example of how you can use it:\n",
    "```python\n",
    "from mleko.dataset.ingest import S3Ingester\n",
    "\n",
    "csv_paths = S3Ingester(\n",
    "    destination_directory=\"data\",\n",
    "    s3_bucket_name=\"mleko-datasets\",\n",
    "    s3_key_prefix=\"kaggle/nehaprabhavalkar/indian-food-101\",\n",
    "    aws_profile_name=\"mleko\",\n",
    "    aws_region_name=\"eu-west-1\",\n",
    "    num_workers=64,  # Number of workers to use for downloading files.\n",
    "    check_s3_timestamps=True,  # Ensure that all files are from the same date.\n",
    ").fetch_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "Here, we use the `CSVToVaexConverter` from `mleko` to clean the data. \n",
    "\n",
    "The converter reads the CSV file, drops unnecessary columns, handles missing values, and converts the data into a Vaex DataFrame for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.dataset.convert import CSVToVaexConverter\n",
    "\n",
    "\n",
    "clean_schema, clean_df = CSVToVaexConverter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/converted\",\n",
    "    drop_columns=[\"Ticket\"],\n",
    "    meta_columns=META_FEATURES,\n",
    "    drop_rows_with_na_columns=[TARGET_FEATURE],  # Drop rows with missing target values\n",
    "    random_state=RANDOM_STATE,  # We like reproducibility\n",
    ").convert(csv_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the data to see what columns are available and what their data types are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_schema.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Val and Test Dataset\n",
    "\n",
    "In this section, we split the cleaned data into a training/validation set and a test set. \n",
    "\n",
    "We use the `RandomSplitter` from `mleko` to perform a stratified random split, ensuring that both sets have the same proportion of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.dataset.split import RandomSplitter\n",
    "\n",
    "\n",
    "clean_train_val_df, clean_test_df = RandomSplitter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/split\",\n",
    "    data_split=(0.90, 0.10),  # 90% train/val, 10% test\n",
    "    shuffle=True,  # Shuffle the data before splitting\n",
    "    stratify=TARGET_FEATURE,  # Stratify on the target feature\n",
    "    random_state=RANDOM_STATE,  # We like reproducibility\n",
    ").split(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the class balance is maintained in the train/val and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_split_stats(df, split_name):\n",
    "    total_count = df.shape[0]\n",
    "    survival_count = df[TARGET_FEATURE].sum()  # type: ignore\n",
    "    survival_rate = survival_count / total_count\n",
    "\n",
    "    print(f\"{split_name}: {survival_rate * 100:.3f}% (Survived: {survival_count:3d}, Total: {total_count:3d})\")\n",
    "\n",
    "\n",
    "print_split_stats(clean_train_val_df, \"Train/Val\")\n",
    "print_split_stats(clean_test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Based on Boolean Expressions\n",
    "\n",
    "For more complex splits, you can use the `ExpressionSplitter` from `mleko` to split the data based on a given boolean expression.\n",
    "\n",
    "It is suitable for splitting data based on time, location, or any other condition like the one below:\n",
    "```python\n",
    "from mleko.dataset.split import ExpressionSplitter\n",
    "\n",
    "train_val_df, test_df = ExpressionSplitter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/split\",\n",
    "    expression=\"(Embarked == 'S') | ((Embarked == 'C') & (Fare < 50))\"  # Train/val set contains passengers who embarked from Southampton or Cherbourg and paid less than 50.\n",
    ").split(clean_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Transformation \n",
    "In this section, we perform feature engineering and transformation using custom transformers, predefined transformers and combining them inside a `CompositeTransformer` from `mleko`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers\n",
    "It is important the the ML pipeline is flexible and allows for easy experimentation, be it with different feature engineering techniques or different models.\n",
    "\n",
    "In many cases, the classes provided by `mleko` will be sufficient for your needs. However, you can also create your own custom classes by inheriting from the `BaseClass` class. For transformers, you need to inherit from the `BaseTransformer` class and implement the `__init__`, `_fit`, `_transform`, and `_fingerprint` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Hashable\n",
    "\n",
    "import vaex\n",
    "import vaex.ml\n",
    "\n",
    "from mleko.dataset import DataSchema\n",
    "from mleko.dataset.transform import BaseTransformer\n",
    "from mleko.utils import auto_repr\n",
    "\n",
    "\n",
    "class IsAloneTransformer(BaseTransformer):\n",
    "    @auto_repr\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_directory: str | Path,\n",
    "        cache_size: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__(cache_directory, [], cache_size)\n",
    "        self._transformer = None\n",
    "\n",
    "    def _fit(self, data_schema: DataSchema, _dataframe: vaex.DataFrame) -> tuple[DataSchema, None]:\n",
    "        \"\"\"No fitting required for this transformer.\"\"\"\n",
    "        ds = data_schema.copy().add_feature(\"IsAlone\", \"boolean\")\n",
    "        return ds, self._transformer\n",
    "\n",
    "    def _transform(self, data_schema: DataSchema, dataframe: vaex.DataFrame) -> tuple[DataSchema, vaex.DataFrame]:\n",
    "        \"\"\"Add a new feature to the dataset indicating whether the passenger was alone or not.\"\"\"\n",
    "        df = dataframe.copy()\n",
    "        df[\"IsAlone\"] = df[\"SibSp\"] + df[\"Parch\"] == 0  # type: ignore\n",
    "        ds = data_schema.copy().add_feature(\"IsAlone\", \"boolean\")\n",
    "        return ds, df\n",
    "\n",
    "    def _fingerprint(self) -> Hashable:\n",
    "        return super()._fingerprint()\n",
    "\n",
    "\n",
    "class FeatureDropperTransformer(BaseTransformer):\n",
    "    @auto_repr\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_directory: str | Path,\n",
    "        features: list[str] | tuple[str, ...],\n",
    "        cache_size: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__(cache_directory, features, cache_size)\n",
    "        self._transformer = None\n",
    "\n",
    "    def _fit(self, data_schema: DataSchema, _dataframe: vaex.DataFrame) -> tuple[DataSchema, None]:\n",
    "        \"\"\"No fitting required for this transformer.\"\"\"\n",
    "        ds = data_schema.copy().drop_features(self._features)\n",
    "        return ds, self._transformer\n",
    "\n",
    "    def _transform(self, data_schema: DataSchema, dataframe: vaex.DataFrame) -> tuple[DataSchema, vaex.DataFrame]:\n",
    "        \"\"\"Drop the specified features from the dataset.\"\"\"\n",
    "        df = dataframe.drop(self._features, inplace=False)\n",
    "        ds = data_schema.copy().drop_features(self._features)\n",
    "        return ds, df\n",
    "\n",
    "    def _fingerprint(self) -> Hashable:\n",
    "        return super()._fingerprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers can be applied individually or combined into a `CompositeTransformer`. The `CompositeTransformer` allows you to combine multiple transformers into a single transformer, saving you from having to apply each transformer individually, like a small pipeline. The custom transformers can be used in the same way as the predefined ones, e.g. `LabelEncoderTransformer` or `FrequencyEncoderTransformer`.\n",
    "\n",
    "The transformers follows the common `fit` and `transform` pattern, similar to `scikit-learn`. This is true for feature selectors and models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.dataset.transform import CompositeTransformer, LabelEncoderTransformer\n",
    "\n",
    "\n",
    "composite_transformer = CompositeTransformer(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "    transformers=[\n",
    "        FeatureDropperTransformer(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "            features=[\"Name\"],\n",
    "        ),\n",
    "        IsAloneTransformer(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "        ),\n",
    "        LabelEncoderTransformer(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "            features=[\"Sex\", \"Embarked\", \"IsAlone\"],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "transform_schema, _, transform_train_val_df = composite_transformer.fit_transform(clean_schema, clean_train_val_df)\n",
    "_, transform_test_df = composite_transformer.transform(clean_schema, clean_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the transformed dataset has correct data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Here, we use the `CompositeFeatureSelector` from `mleko` to select the most relevant features for our model. We use three selectors: `MissingRateFeatureSelector` to remove features with too many missing values, `InvarianceFeatureSelector` to remove invariant features, and `PearsonCorrelationFeatureSelector` to remove highly correlated features. We also display a correlation matrix for the selected numerical features.\n",
    "\n",
    "Just like transformers, feature selectors can be applied individually or combined into a `CompositeFeatureSelector`, and allow for custom feature selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.dataset.feature_select import (\n",
    "    CompositeFeatureSelector,\n",
    "    InvarianceFeatureSelector,\n",
    "    MissingRateFeatureSelector,\n",
    "    PearsonCorrelationFeatureSelector,\n",
    ")\n",
    "\n",
    "\n",
    "composite_feature_selector = CompositeFeatureSelector(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "    feature_selectors=[\n",
    "        MissingRateFeatureSelector(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "            missing_rate_threshold=0.5,\n",
    "            ignore_features=META_FEATURES,\n",
    "        ),\n",
    "        InvarianceFeatureSelector(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "            ignore_features=META_FEATURES,\n",
    "        ),\n",
    "        PearsonCorrelationFeatureSelector(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "            correlation_threshold=0.7,\n",
    "            ignore_features=META_FEATURES,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "data_schema, _, feature_select_train_val_df = composite_feature_selector.fit_transform(\n",
    "    transform_schema, transform_train_val_df\n",
    ")\n",
    "_, test_df = composite_feature_selector.transform(transform_schema, transform_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Cabin` feature has too many missing values, so we drop it. No other feature was dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_select_train_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "We further split our training/validation data into a training set and a validation set. The `LGBMModel` is trained on the training set and evaluated on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = RandomSplitter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/split\",\n",
    "    data_split=(0.80, 0.20),\n",
    "    shuffle=True,\n",
    "    stratify=TARGET_FEATURE,\n",
    "    random_state=RANDOM_STATE,\n",
    ").split(feature_select_train_val_df, cache_group=\"train_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the class balance is maintained in the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_split_stats(train_df, \"Train\")\n",
    "print_split_stats(val_df, \"Val\")\n",
    "print_split_stats(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and evaluate it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.model import LGBMModel\n",
    "\n",
    "\n",
    "lgbm_model = LGBMModel(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/model\",\n",
    "    objective=\"binary\",\n",
    "    target=TARGET_FEATURE,\n",
    "    num_iterations=100,\n",
    "    ignore_features=META_FEATURES,\n",
    "    metric=[\"average_precision\", \"auc\"],\n",
    ")\n",
    "\n",
    "model, metrics, p_train_df, p_val_df = lgbm_model.fit_transform(data_schema, train_df, val_df, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "\n",
    "ax = lightgbm.plot_metric(metrics, metric=\"auc\")\n",
    "ax = lightgbm.plot_metric(metrics, metric=\"average_precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mleko` Pipeline\n",
    "\n",
    "The `mleko` pipeline is used to streamline the entire process. Pipelines are very flexible and allows users to define a directed acyclic graph (DAG) of operations. You can chain together all operations in a single pipeline or create multiple pipelines for different tasks.\n",
    "\n",
    "We create two pipelines: \n",
    "- a pre-processing pipeline that handles data ingestion, conversion, splitting, transformation, and feature selection\n",
    "- a model pipeline that trains and evaluates the model.\n",
    "\n",
    "Next we define all required classes to create the pre-processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_ingester = KaggleIngester(\n",
    "    destination_directory=f\"data/{DATASET_NAME}/raw\", \n",
    "    owner_slug=OWNER_SLUG, \n",
    "    dataset_slug=DATASET_SLUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_vaex_converter = CSVToVaexConverter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/converted\",\n",
    "    drop_columns=[\"Ticket\"],\n",
    "    meta_columns=META_FEATURES,\n",
    "    drop_rows_with_na_columns=[TARGET_FEATURE],\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_splitter_90_10 = RandomSplitter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/split\",\n",
    "    data_split=(0.90, 0.10),\n",
    "    shuffle=True,\n",
    "    stratify=TARGET_FEATURE,\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_transformer = CompositeTransformer(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "    transformers=[\n",
    "        FeatureDropperTransformer(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "            features=[\"Name\"],\n",
    "        ),\n",
    "        IsAloneTransformer(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "        ),\n",
    "        LabelEncoderTransformer(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/transform\",\n",
    "            features=[\"Sex\", \"Embarked\", \"IsAlone\"],\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_feature_selector = CompositeFeatureSelector(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "    feature_selectors=[\n",
    "        MissingRateFeatureSelector(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "            missing_rate_threshold=0.5,\n",
    "            ignore_features=META_FEATURES,\n",
    "        ),\n",
    "        InvarianceFeatureSelector(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "            ignore_features=META_FEATURES,\n",
    "        ),\n",
    "        PearsonCorrelationFeatureSelector(\n",
    "            cache_directory=f\"data/{DATASET_NAME}/feature_select\",\n",
    "            correlation_threshold=0.7,\n",
    "            ignore_features=META_FEATURES,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_splitter_80_20 = RandomSplitter(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/split\",\n",
    "    data_split=(0.80, 0.20),\n",
    "    shuffle=True,\n",
    "    stratify=TARGET_FEATURE,\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pre-processing pipeline performing all the dataset pre-processing steps before training the model.\n",
    "\n",
    "Each `PipelineStep` accepts a class, a list of input names, and a list of output names. The class is used to instantiate the step, the input names are used to fetch the required data from the previous steps, and the output names are used to store the output of the step for use by subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.pipeline import Pipeline\n",
    "from mleko.pipeline.steps import ConvertStep, FeatureSelectStep, IngestStep, SplitStep, TransformStep\n",
    "\n",
    "\n",
    "pre_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        IngestStep(kaggle_ingester, outputs=[\"raw_csv\"]),\n",
    "        ConvertStep(csv_to_vaex_converter, inputs=[\"raw_csv\"], outputs=[\"clean_data_schema\", \"clean_df\"]),\n",
    "        SplitStep(\n",
    "            random_splitter_90_10,\n",
    "            inputs=[\"clean_df\"],\n",
    "            outputs=[\"train_val_clean_df\", \"test_clean_df\"],\n",
    "            cache_group=\"train_val_test\",\n",
    "        ),\n",
    "        TransformStep(\n",
    "            composite_transformer,\n",
    "            action=\"fit_transform\",\n",
    "            inputs=[\"clean_data_schema\", \"train_val_clean_df\"],\n",
    "            outputs=[\"transform_data_schema\", \"composite_transformer\", \"transform_train_val_df\"],\n",
    "            cache_group=\"train_val\",\n",
    "        ),\n",
    "        TransformStep(\n",
    "            composite_transformer,\n",
    "            action=\"transform\",\n",
    "            inputs=[\"clean_data_schema\", \"test_clean_df\"],\n",
    "            outputs=[\"transform_data_schema\", \"transform_test_df\"],\n",
    "            cache_group=\"test\",\n",
    "        ),\n",
    "        FeatureSelectStep(\n",
    "            composite_feature_selector,\n",
    "            action=\"fit_transform\",\n",
    "            inputs=[\"transform_data_schema\", \"transform_train_val_df\"],\n",
    "            outputs=[\"data_schema\", \"composite_feature_selector\", \"selected_train_val_df\"],\n",
    "            cache_group=\"train_val\",\n",
    "        ),\n",
    "        FeatureSelectStep(\n",
    "            composite_feature_selector,\n",
    "            action=\"transform\",\n",
    "            inputs=[\"transform_data_schema\", \"transform_test_df\"],\n",
    "            outputs=[\"data_schema\", \"test_df\"],\n",
    "            cache_group=\"test\",\n",
    "        ),\n",
    "        SplitStep(\n",
    "            random_splitter_80_20,\n",
    "            inputs=[\"selected_train_val_df\"],\n",
    "            outputs=[\"train_df\", \"val_df\"],\n",
    "            cache_group=\"train_val\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `Pipeline` steps to see the order in which they will be executed, for double-checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the pre-processing pipeline and store the output in `pre_data_container`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_container = pre_pipeline.run(force_recompute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleko.pipeline.steps import ModelStep\n",
    "\n",
    "\n",
    "lgbm_model = LGBMModel(\n",
    "    cache_directory=f\"data/{DATASET_NAME}/model\",\n",
    "    objective=\"binary\",\n",
    "    num_leaves=11,\n",
    "    target=TARGET_FEATURE,\n",
    "    num_iterations=100,\n",
    "    ignore_features=META_FEATURES,\n",
    "    metric=[\"average_precision\", \"auc\"],\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ModelStep(\n",
    "            lgbm_model,\n",
    "            action=\"fit_transform\",\n",
    "            inputs=[\"data_schema\", \"train_df\", \"val_df\"],\n",
    "            outputs=[\"lgbm_model\", \"metrics\", \"pred_train_df\", \"pred_val_df\"],\n",
    "        ),\n",
    "        ModelStep(\n",
    "            lgbm_model,\n",
    "            action=\"transform\",\n",
    "            inputs=[\"data_schema\", \"test_df\"],\n",
    "            outputs=[\"pred_test_df\"],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model pipeline by feeding the output of the pre-processing pipeline into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = model_pipeline.run(data_container=pre_data_container)\n",
    "result = data_container.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results from each step are stored in the `data_container.data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lightgbm.plot_metric(result[\"metrics\"], metric='auc')\n",
    "ax = lightgbm.plot_metric(result[\"metrics\"], metric='average_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
